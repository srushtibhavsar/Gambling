{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0uGDUo+QSssgYATxWl3ca",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12167b7b4f0f4d05a1249b1d30d6e18d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8142b9fc6d14ffa9ddbe2e6d48f60cf",
              "IPY_MODEL_cc997f24561b4925a4499e49a2aee9a4",
              "IPY_MODEL_b3448bbfb6a447ec8c397ce4a9eaf12a"
            ],
            "layout": "IPY_MODEL_fec92d2ae9b747dba9ec67acf64f46a1"
          }
        },
        "b8142b9fc6d14ffa9ddbe2e6d48f60cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dcfcd085e874a96b753e9ec1faae38a",
            "placeholder": "​",
            "style": "IPY_MODEL_ca13a28e03d047288bd60cb5b5ebdd1d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cc997f24561b4925a4499e49a2aee9a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d658804fbd14a85a3b4b78b9f80f46c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_02646a72e21e4b59a5118d6f348d01c3",
            "value": 2
          }
        },
        "b3448bbfb6a447ec8c397ce4a9eaf12a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa70b85b0b2341ee9f2ccdceae76a108",
            "placeholder": "​",
            "style": "IPY_MODEL_d2320184837c4b95a53ddac871418fea",
            "value": " 2/2 [00:54&lt;00:00, 24.80s/it]"
          }
        },
        "fec92d2ae9b747dba9ec67acf64f46a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dcfcd085e874a96b753e9ec1faae38a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca13a28e03d047288bd60cb5b5ebdd1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d658804fbd14a85a3b4b78b9f80f46c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02646a72e21e4b59a5118d6f348d01c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa70b85b0b2341ee9f2ccdceae76a108": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2320184837c4b95a53ddac871418fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75cf1e982b044c09b84462a75b1309a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_469745936b2846a998ffcda8015b7391",
              "IPY_MODEL_3108e88b6be841988cd3df0629789c79",
              "IPY_MODEL_463d441c1b6a4f16816fa81440b18593"
            ],
            "layout": "IPY_MODEL_4023ee8e554d4b4d9e73e2c0eb63b97c"
          }
        },
        "469745936b2846a998ffcda8015b7391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_712d72ab6eed41389b2d9111aeeed42c",
            "placeholder": "​",
            "style": "IPY_MODEL_ea45a082adec4483b900adc0c6b6dc2b",
            "value": "Downloading builder script: 100%"
          }
        },
        "3108e88b6be841988cd3df0629789c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66014ce9a8b04772b1f8761d2f93492e",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc2bd4bfd93e46528b6eec7a472856f4",
            "value": 6270
          }
        },
        "463d441c1b6a4f16816fa81440b18593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5b29cf484674d06ac51ad019bbcb166",
            "placeholder": "​",
            "style": "IPY_MODEL_e4efaa120c1a45b39f47a3de592726d9",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 246kB/s]"
          }
        },
        "4023ee8e554d4b4d9e73e2c0eb63b97c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "712d72ab6eed41389b2d9111aeeed42c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea45a082adec4483b900adc0c6b6dc2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66014ce9a8b04772b1f8761d2f93492e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc2bd4bfd93e46528b6eec7a472856f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5b29cf484674d06ac51ad019bbcb166": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4efaa120c1a45b39f47a3de592726d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srushtibhavsar/Gambling/blob/main/Generative_AI_Seminar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhXPitxI8M3c",
        "outputId": "e19fe0cf-9ae4-4510-94ad-093977828037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q transformers einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "id": "BJprWpNV8Xm_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJOExruA9VbE",
        "outputId": "921f59d4-c01d-4fe9-c834-90f7270fb5c9",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.6.20)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BYsbjRZ39VfN",
        "outputId": "9156b54f-0965-498e-9e74-b5536acc3b26"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama_index in /usr/local/lib/python3.10/dist-packages (0.10.65)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.2.9)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.13)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.65 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.10.67)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.11)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.2.7)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.9.48.post3)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.27 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.29)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.9)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.7)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.33)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.6)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama_index) (1.41.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.65->llama_index) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (3.10.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (3.3)\n",
            "Requirement already satisfied: nltk!=3.9,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (2.1.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (1.16.0)\n",
            "Requirement already satisfied: llama-cloud>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from llama-index-indices-managed-llama-cloud>=0.2.0->llama_index) (0.0.13)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.3.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse>=0.1.2->llama_index) (0.4.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama_index) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama_index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama_index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama_index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama_index) (2.8.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama_index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama_index) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama_index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.65->llama_index) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.65->llama_index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.65->llama_index) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama_index) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama_index) (0.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.65->llama_index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.65->llama_index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.65->llama_index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.65->llama_index) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.65->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.65->llama_index) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.65->llama_index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.65->llama_index) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama_index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama_index) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install llama-index --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6Lo2MRP-tN_",
        "outputId": "90224777-c355-4b3a-bb41-63e3e3b74574"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.10.65)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.9)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.13)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.65 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.10.67)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.11)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.7)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.48.post3)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.27 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.29)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.9)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.7)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.33)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.41.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.65->llama-index) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (3.10.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (3.3)\n",
            "Requirement already satisfied: nltk!=3.9,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (2.1.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (1.16.0)\n",
            "Requirement already satisfied: llama-cloud>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (0.0.13)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.3.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse>=0.1.2->llama-index) (0.4.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama-index) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama-index) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.65->llama-index) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.65->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.65->llama-index) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (0.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.65->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.65->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.65->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.65->llama-index) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.65->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.65->llama-index) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.65->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.65->llama-index) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0hPMtVyWBBPG",
        "outputId": "44b1ece8-8673-4066-bb8f-47148c4f445d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-llms-huggingface in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.23.5)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.57 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.10.67)\n",
            "Requirement already satisfied: text-generation<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (2.3.1+cu121)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.12.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (3.10.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk!=3.9,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.41.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2.1.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (9.4.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.16.0)\n",
            "Requirement already satisfied: pydantic<3,>2 in /usr/local/lib/python3.10/dist-packages (from text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.8.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.6.20)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.19.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.32.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (3.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (3.21.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.57->llama-index-llms-huggingface) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext"
      ],
      "metadata": {
        "id": "CNSNglJ09Vii"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbAHHXxDBIYv",
        "outputId": "f590f095-faf9-4bc9-e7ea-fa0cbc648cb0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.prompts.prompts import SimpleInputPrompt"
      ],
      "metadata": {
        "id": "s_CzV2ObBLrq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create SimpleDirectoryReader object and read documents\n",
        "reader = SimpleDirectoryReader(input_dir='/content/data')\n",
        "documents = reader.load_data()  # Correct way to load documents"
      ],
      "metadata": {
        "id": "g4XcPwNaB1Ym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6e4284b-647a-4c74-9553-24fe35edc9c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf.generic._base:could not convert string to float: '0.00-20' : FloatObject (b'0.00-20') invalid; use 0.0 instead\n",
            "WARNING:pypdf.generic._base:could not convert string to float: '0.00-20' : FloatObject (b'0.00-20') invalid; use 0.0 instead\n",
            "WARNING:pypdf.generic._base:could not convert string to float: '0.00-20' : FloatObject (b'0.00-20') invalid; use 0.0 instead\n",
            "WARNING:pypdf.generic._base:could not convert string to float: '0.00-20' : FloatObject (b'0.00-20') invalid; use 0.0 instead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\""
      ],
      "metadata": {
        "id": "H_n4lZsoCBMv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_wrap = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "XSr4gyyuCBQM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli login #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"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfHMXvPfDK9q",
        "outputId": "5693e71f-0691-4050-d43f-bedb5413611d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) \n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xEO8LUakWNus"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrap,\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map=\"auto\",\n",
        "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "12167b7b4f0f4d05a1249b1d30d6e18d",
            "b8142b9fc6d14ffa9ddbe2e6d48f60cf",
            "cc997f24561b4925a4499e49a2aee9a4",
            "b3448bbfb6a447ec8c397ce4a9eaf12a",
            "fec92d2ae9b747dba9ec67acf64f46a1",
            "8dcfcd085e874a96b753e9ec1faae38a",
            "ca13a28e03d047288bd60cb5b5ebdd1d",
            "3d658804fbd14a85a3b4b78b9f80f46c",
            "02646a72e21e4b59a5118d6f348d01c3",
            "fa70b85b0b2341ee9f2ccdceae76a108",
            "d2320184837c4b95a53ddac871418fea"
          ]
        },
        "id": "Tv88OYnhDLAc",
        "outputId": "5c073ec5-d6f9-4029-8ee5-5769b82a5940"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12167b7b4f0f4d05a1249b1d30d6e18d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43BsfDhHHOlb",
        "outputId": "9c763944-f921-427f-dce8-ab627c1349f9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.2.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.14)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.99)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.4.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "2DLhvbLBGSZ-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import ServiceContext"
      ],
      "metadata": {
        "id": "UsUa4eSZGSlH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-embeddings-langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4liL8c5IqEz",
        "outputId": "e9700105-ce0a-4c51-bb5a-0ba793daf643",
        "collapsed": true
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-embeddings-langchain in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-langchain) (0.10.67)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.10.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.3)\n",
            "Requirement already satisfied: nltk!=3.9,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.41.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.1.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.5.15)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.langchain import LangchainEmbedding"
      ],
      "metadata": {
        "id": "6EIUO_vbH6Xl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxSCwEpMIxC2",
        "outputId": "15b4f76e-9d94-4783-fad1-ef592a459b1c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdg7Vcd2JZj5",
        "outputId": "ae6a42c3-0d65-49a8-c9ff-7b2705a6fd9e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-6394536e40c5>:1: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9znY5BTPIxLZ",
        "outputId": "c5649e5f-dfdb-4a24-cdf1-573a97b486c8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=LangchainEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7986a4ccd7b0>, num_workers=None), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7986a4ccd7b0>, id_func=<function default_id_func at 0x79880982b400>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.core.service_context_elements.llama_logger.LlamaLogger object at 0x798694d427a0>, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7986a4ccd7b0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
      ],
      "metadata": {
        "id": "IlhAE5OxJ0W8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waS5cjMJWPnx",
        "outputId": "cb849713-da69-4197-b4af-a2d3112bd06c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x798848133280>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.as_query_engine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rupD2J3WYai",
        "outputId": "e5c6e9be-5e66-4eca-85cc-3682d3557c94"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x79874e0422c0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "kJcG89qWWgZ0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What role does Bloom's Taxonomy play in the Angel approach? \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqRCa_wRWgch",
        "outputId": "84e98694-903e-437c-fa1e-1ee28a7380b7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scW9j22FY45u",
        "outputId": "a1e26bd4-2d5e-4e55-d538-1c13f0a5acfd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(response=\"Bloom's Taxonomy plays a crucial role in the Angel approach as it provides a framework for creating questions that assess different levels of cognitive thinking. The Angel approach emphasizes the importance of assessing higher-order thinking skills, such as analyzing, evaluating, and creating, which are aligned with the different levels of Bloom's Taxonomy. By using Bloom's Taxonomy, educators can design questions that encourage students to think critically and deeply about the subject matter, rather than simply recalling facts or information. Additionally, the Angel approach emphasizes the use of open-ended questions, which are also aligned with Bloom's Taxonomy as they require students to demonstrate their understanding through creative and innovative responses. Overall, Bloom's Taxonomy is an essential component of the Angel approach, as it provides a structure for creating questions that promote deeper learning and critical thinking.\", source_nodes=[NodeWithScore(node=TextNode(id_='da4b71a5-f870-4186-ae3e-4199d264d3bc', embedding=None, metadata={'page_label': '13', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e2818949-5678-4a4c-b374-0cf25ff3d7b4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '13', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, hash='f2ded61ab0902349025070cee2570ea50006657e5fb07a28c61f15779990f53f')}, text='3.For each question, identify its main Bloom Taxonomy Learning Objective. If the question\\ncovers several learning objectives, specify the main one. Assume that the following are the\\nBloom Taxonomy learning objectives: <Here we give the same definition of the Bloom\\nTaxonomy given to the GPT model>.\\n13', mimetype='text/plain', start_char_idx=0, end_char_idx=301, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5398879175474435), NodeWithScore(node=TextNode(id_='73c0faf5-eef0-42c6-a26a-7c96e0799a58', embedding=None, metadata={'page_label': '12', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0a722e91-a190-44a4-a693-a7e5a73acfcd', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '12', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, hash='ee10daf410eb2cbf4667af044c071af04f73a60160e0a78e8038bd9307c9cc2c')}, text='B.3 Bloom Taxonomy Level Evaluation\\nYou are an experienced educator for mid school pupils. Your task is to receive questions\\nwritten by a junior teacher for a given paragraph and to grade the questions based on the\\nBloom Taxonomy. For each bloom taxonomy learning objective you should give a score of\\n1-5 and an explanation of why you chose that score.\\nUse this scoring scale:\\n1- Minimal or No Coverage: A rating of \"1\" indicates that the educational question\\nprovides minimal or no coverage of the specified level of Bloom’s Taxonomy. The question\\ndoes not engage learners in thinking at that cognitive level.\\n2- Limited Coverage: A rating of \"2\" suggests that the question includes some ele-\\nments or cues related to the specified Bloom’s Taxonomy level, but it does not fully or\\neffectively engage learners at that level.\\n3- Partial Coverage: A rating of \"3\" implies that the question partially addresses\\nthe specified Bloom’s Taxonomy level. It involves some elements of thinking or skills\\nassociated with that level but may lack depth or complexity.\\n4- Adequate Coverage: A rating of \"4\" indicates that the educational question ade-\\nquately covers the specified Bloom’s Taxonomy level. It engages learners in thinking and\\ntasks characteristic of that level, providing a reasonably challenging cognitive experience.\\n5- Comprehensive Coverage: A rating of \"5\" signifies that the question comprehen-\\nsively and effectively covers the specified level of Bloom’s Taxonomy. It engages learners in\\ndeep, complex thinking and problem-solving aligned with that level.\\nAssume that the following are the Bloom Taxonomy learning objectives:\\nRemembering: This level involves recalling facts, details, or information. It is the most basic\\nlevel of cognitive skill.\\nUnderstanding: At this level, learners comprehend the meaning of information and can\\nexplain it in their own words. They demonstrate comprehension of concepts and principles\\nApplying: This level requires the application of knowledge and concepts to solve problems\\nor perform tasks. It involves using information in new and different situations.\\nAnalyzing: Analyzing involves breaking down information into its constituent parts and\\nidentifying patterns, relationships, or connections among them.\\nEvaluating: At this level, learners make judgments about the value or quality of ideas,\\nsolutions, or arguments. They can critique, assess, and defend their positions.\\nCreating: This is the highest level of cognitive skill, where learners can generate new ideas,\\nconcepts, or products. They can synthesize information from different sources to create\\nsomething new.\\nThe Paragraph: {{Paragraph}}\\nThe Questions for this paragraph: {{Question created for the paragraph}}\\nFigure 7: Bloom Taxonomy Evaluation Prompt\\nB.4 Questions for Human Evaluator\\n1.Rate each question as either pedagogically sound (score=1) or not (score=0). A pedagogi-\\ncally sound question is one that pertains to the paragraph content and is intended to assess\\nthe domain knowledge of the student. A question is classified as not sound if it is vague,\\nunclear, or not about assessing domain knowledge (similar to [16]).\\n2.Rate each answer as either correct given the paragraph text and the question (score=1) or\\nincorrect (score=0).\\n12', mimetype='text/plain', start_char_idx=0, end_char_idx=3255, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.46544039703957396)], metadata={'da4b71a5-f870-4186-ae3e-4199d264d3bc': {'page_label': '13', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, '73c0faf5-eef0-42c6-a26a-7c96e0799a58': {'page_label': '12', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wKonzBlY8un",
        "outputId": "d0e88b2c-796c-4507-aadf-cc38e67f8877"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bloom's Taxonomy plays a crucial role in the Angel approach as it provides a framework for creating questions that assess different levels of cognitive thinking. The Angel approach emphasizes the importance of assessing higher-order thinking skills, such as analyzing, evaluating, and creating, which are aligned with the different levels of Bloom's Taxonomy. By using Bloom's Taxonomy, educators can design questions that encourage students to think critically and deeply about the subject matter, rather than simply recalling facts or information. Additionally, the Angel approach emphasizes the use of open-ended questions, which are also aligned with Bloom's Taxonomy as they require students to demonstrate their understanding through creative and innovative responses. Overall, Bloom's Taxonomy is an essential component of the Angel approach, as it provides a structure for creating questions that promote deeper learning and critical thinking.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = query_engine.query(\"Evaluate the impact of using generative AI models like Angel on educational assessment.\")"
      ],
      "metadata": {
        "id": "VZM7nSlDazmf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN7DwBiUaztm",
        "outputId": "3bb97b9f-ae26-4348-fe07-313ba32d6691"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The impact of using generative AI models like Angel on educational assessment is a complex and multifaceted topic. While there are potential benefits to using these models, such as increased efficiency and cost-effectiveness, there are also potential drawbacks, such as the loss of human judgment and the potential for bias in the assessment process.\n",
            "\n",
            "To fully evaluate the impact of generative AI models like Angel on educational assessment, it is important to conduct thorough research and analysis, taking into account the specific context and application. This may involve comparing the performance of generative AI models like Angel with traditional assessment methods, as well as examining the potential impact on student learning outcomes and overall educational quality.\n",
            "\n",
            "In general, it is important to approach the use of generative AI models like Angel in educational assessment with caution and careful consideration, in order to ensure that they are used in a way that is fair, transparent, and aligned with the goals of education.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXUbFxOth2sw",
        "outputId": "97431d88-326d-424b-cd9b-8fab4ecdca08"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(response='The impact of using generative AI models like Angel on educational assessment is a complex and multifaceted topic. While there are potential benefits to using these models, such as increased efficiency and cost-effectiveness, there are also potential drawbacks, such as the loss of human judgment and the potential for bias in the assessment process.\\n\\nTo fully evaluate the impact of generative AI models like Angel on educational assessment, it is important to conduct thorough research and analysis, taking into account the specific context and application. This may involve comparing the performance of generative AI models like Angel with traditional assessment methods, as well as examining the potential impact on student learning outcomes and overall educational quality.\\n\\nIn general, it is important to approach the use of generative AI models like Angel in educational assessment with caution and careful consideration, in order to ensure that they are used in a way that is fair, transparent, and aligned with the goals of education.', source_nodes=[NodeWithScore(node=TextNode(id_='210a3503-748e-4399-850a-a4e87707f064', embedding=None, metadata={'page_label': '1', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='feb58b53-c358-4835-ba9e-adb4f2acb02b', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, hash='51ff496876eb576301f2ab9e5fc99dd9948b9cf64833754783a1193ea0e87ffc')}, text='Angel: A New Generation Tool for Learning Material\\nbased Questions and Answers\\nAriel Blobstein∗\\nBen-Gurion University\\narielblo@post.bgu.ac.ilDaniel Izmaylov∗\\nBen-Gurion University\\nizmaylov@post.bgu.ac.il\\nTal Yifal\\nData Innovation Lab\\ntal@datainnovationlab.aiMichal Levy\\nTel Aviv University\\nmichalyot@gmail.comAvi Segal\\nBen-Gurion University\\navise@post.bgu.ac.il\\nAbstract\\nCreating high quality questions and answers for educational purposes continues to\\nbe a challenge for educators and publishers. Past attempts to address this through\\nautomatic generation have shown limited abilities to generate questions targeting\\nhigh cognitive levels, control question complexity and difficulty, or create adequate\\nquestion-answer pairs. We take first steps toward addressing these limitations\\nby introducing a new approach, named Angel, informed by recent developments\\nin Large Language Models and Generative AI. Relying on advanced prompting\\ntechniques, automatic curation, and the incorporation of educational theory into\\nprompts, Angel focuses on generating question answer pairs of varied difficulty\\nwhile targeting higher cognitive levels. Questions and answers are automatically\\ngenerated based on a textbook extract, with Bloom Taxonomy serving as a guide\\nto the creation of questions addressing a diverse set of learning objectives. Our\\nexperiments compare Angel to several baselines and demonstrate the potential of\\ninformed generative models to create high-quality question answer pairs that cover\\na diverse range of cognitive skills.\\n1 Introduction\\nGenerating high-quality questions and answers is of paramount importance in the educational sector.\\nExam-style questions serve as an essential educational tool for assessment as well as a catalyst\\nfor student learning. They provide opportunities for students to practice retrieving information,\\nfocus attention on key learning materials, reinforce learning through repetition of core concepts, and\\nmotivate engagement in learning activities [ 21]. However, manually creating these questions is a\\ncomplex task, requiring significant resources, training, and experience. Automatic questions and\\nanswer generation techniques have emerged as a potential solution to these challenges, offering the\\npossibility of constructing high-quality questions efficiently and cost-effectively. However, until\\nrecently, these techniques have primarily employed a template-based methods. Past reviews highlight\\nthe limitations of these approaches, particularly their inability to generate questions that target high\\ncognitive levels, control question complexity and difficulty, or provide constructive feedback to\\nthe learner [ 26]. Furthermore, these approaches have been constrained by their tendency to rely\\non the existence of a pre-formulated answer for the generated question [ 4,5]. Recent advances in\\nGenerative AI, known for their exceptional text generation capabilities, offer still untapped potential\\nfor automatic question and answer generation [2].\\n∗Equal contribution: These authors contributed equally to this work.\\nNeurIPS’23 Workshop on Generative AI for Education (GAIED).', mimetype='text/plain', start_char_idx=0, end_char_idx=3123, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.605921071296362), NodeWithScore(node=TextNode(id_='e66e6149-b9f0-4978-b319-0cd78f4ca507', embedding=None, metadata={'page_label': '7', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4ed650e8-4ef8-4bfa-ba7b-109fc0a26f79', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '7', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, hash='7556e655998f2d7ea451fe13a3106a4f881bb56d82221891fd4c768dbb7a8420'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='642ba759-edbf-4a52-9c6f-8735b01e412d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5e7f8dc0cffc08f509353ab9c15e6096e604e94080c8b44db9e7ce25e6c73eb2')}, text='when augmented with domain expertise. Future work should focus on increasing sample size to check\\nfor statistical significance, providing better human curated examples to the generative process to\\ndemonstrate higher cognitively complex questions, and using multiple LLMs for LLM-based scoring\\nto alleviate for possible bias introduced by a single model.\\nReferences\\n[1]C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins. Synthetic qa corpora generation with\\nroundtrip consistency. arXiv preprint arXiv:1906.05416 , 2019.\\n[2]D. Baidoo-Anu and L. O. Ansah. Education in the era of generative artificial intelligence (ai):\\nUnderstanding the potential benefits of chatgpt in promoting teaching and learning. Journal of\\nAI, 7(1):52–62, 2023.\\n[3]B. S. Bloom and D. R. Krathwohl. Taxonomy of educational objectives: The classification of\\neducational goals. Book 1, Cognitive domain . longman, 1956.\\n[4]X. Du, J. Shao, and C. Cardie. Learning to ask: Neural question generation for reading\\ncomprehension. arXiv preprint arXiv:1705.00106 , 2017.\\n[5]M. Heilman and N. A. Smith. Good question! statistical ranking for question generation. In\\nHuman language technologies: The 2010 annual conference of the North American Chapter of\\nthe Association for Computational Linguistics , pages 609–617, 2010.\\n[6]E. S. Koç and T. Öntas. A comparative analysis of the 4th and 5th grade social studies curriculum\\naccording to revised bloom taxonomy. Cypriot Journal of Educational Sciences , 15(3):540–553,\\n2020.\\n[7]D. R. Krathwohl. A revision of bloom’s taxonomy: An overview. Theory into practice ,\\n41(4):212–218, 2002.\\n[8]G. Kurdi, J. Leo, B. Parsia, U. Sattler, and S. Al-Emari. A systematic review of automatic\\nquestion generation for educational purposes. International Journal of Artificial Intelligence in\\nEducation , 30:121–204, 2020.\\n[9]T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein,\\nI. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering\\nresearch. Transactions of the Association for Computational Linguistics , 7:453–466, 2019.\\n[10] J. Leinonen, P. Denny, S. MacNeil, S. Sarsa, S. Bernstein, J. Kim, A. Tran, and A. Hellas.\\nComparing code explanations created by students and large language models. arXiv preprint\\narXiv:2304.03938 , 2023.\\n[11] X. Li, P. Yu, C. Zhou, T. Schick, L. Zettlemoyer, O. Levy, J. Weston, and M. Lewis. Self-\\nalignment with instruction backtranslation. arXiv preprint arXiv:2308.06259 , 2023.\\n[12] X. Li, T. Zhang, Y . Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto.\\nAlpacaeval: An automatic evaluator of instruction-following models, 2023.\\n[13] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict:\\nA systematic survey of prompting methods in natural language processing. ACM Computing\\nSurveys , 55(9):1–35, 2023.\\n[14] Y . Liu, D. Iter, Y . Xu, S. Wang, R. Xu, and C. Zhu. G-eval: Nlg evaluation using gpt-4 with\\nbetter human alignment. arXiv preprint arXiv:2303.16634 , 2023.\\n[15] T. Muhayimana, L. Kwizera, and M. R. Nyirahabimana.', mimetype='text/plain', start_char_idx=0, end_char_idx=3104, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5818067322602245)], metadata={'210a3503-748e-4399-850a-a4e87707f064': {'page_label': '1', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, 'e66e6149-b9f0-4978-b319-0cd78f4ca507': {'page_label': '7', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res.source_nodes[0].get_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "poZcDKro7whV",
        "outputId": "e9d5af7a-446d-4f96-ee1f-2313f01b00f6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Angel: A New Generation Tool for Learning Material\\nbased Questions and Answers\\nAriel Blobstein∗\\nBen-Gurion University\\narielblo@post.bgu.ac.ilDaniel Izmaylov∗\\nBen-Gurion University\\nizmaylov@post.bgu.ac.il\\nTal Yifal\\nData Innovation Lab\\ntal@datainnovationlab.aiMichal Levy\\nTel Aviv University\\nmichalyot@gmail.comAvi Segal\\nBen-Gurion University\\navise@post.bgu.ac.il\\nAbstract\\nCreating high quality questions and answers for educational purposes continues to\\nbe a challenge for educators and publishers. Past attempts to address this through\\nautomatic generation have shown limited abilities to generate questions targeting\\nhigh cognitive levels, control question complexity and difficulty, or create adequate\\nquestion-answer pairs. We take first steps toward addressing these limitations\\nby introducing a new approach, named Angel, informed by recent developments\\nin Large Language Models and Generative AI. Relying on advanced prompting\\ntechniques, automatic curation, and the incorporation of educational theory into\\nprompts, Angel focuses on generating question answer pairs of varied difficulty\\nwhile targeting higher cognitive levels. Questions and answers are automatically\\ngenerated based on a textbook extract, with Bloom Taxonomy serving as a guide\\nto the creation of questions addressing a diverse set of learning objectives. Our\\nexperiments compare Angel to several baselines and demonstrate the potential of\\ninformed generative models to create high-quality question answer pairs that cover\\na diverse range of cognitive skills.\\n1 Introduction\\nGenerating high-quality questions and answers is of paramount importance in the educational sector.\\nExam-style questions serve as an essential educational tool for assessment as well as a catalyst\\nfor student learning. They provide opportunities for students to practice retrieving information,\\nfocus attention on key learning materials, reinforce learning through repetition of core concepts, and\\nmotivate engagement in learning activities [ 21]. However, manually creating these questions is a\\ncomplex task, requiring significant resources, training, and experience. Automatic questions and\\nanswer generation techniques have emerged as a potential solution to these challenges, offering the\\npossibility of constructing high-quality questions efficiently and cost-effectively. However, until\\nrecently, these techniques have primarily employed a template-based methods. Past reviews highlight\\nthe limitations of these approaches, particularly their inability to generate questions that target high\\ncognitive levels, control question complexity and difficulty, or provide constructive feedback to\\nthe learner [ 26]. Furthermore, these approaches have been constrained by their tendency to rely\\non the existence of a pre-formulated answer for the generated question [ 4,5]. Recent advances in\\nGenerative AI, known for their exceptional text generation capabilities, offer still untapped potential\\nfor automatic question and answer generation [2].\\n∗Equal contribution: These authors contributed equally to this work.\\nNeurIPS’23 Workshop on Generative AI for Education (GAIED).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res.source_nodes[1].get_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "FQPLbeJl75aV",
        "outputId": "5b1ff8a3-82ec-499e-d6c3-e5141a9ed4b2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'when augmented with domain expertise. Future work should focus on increasing sample size to check\\nfor statistical significance, providing better human curated examples to the generative process to\\ndemonstrate higher cognitively complex questions, and using multiple LLMs for LLM-based scoring\\nto alleviate for possible bias introduced by a single model.\\nReferences\\n[1]C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins. Synthetic qa corpora generation with\\nroundtrip consistency. arXiv preprint arXiv:1906.05416 , 2019.\\n[2]D. Baidoo-Anu and L. O. Ansah. Education in the era of generative artificial intelligence (ai):\\nUnderstanding the potential benefits of chatgpt in promoting teaching and learning. Journal of\\nAI, 7(1):52–62, 2023.\\n[3]B. S. Bloom and D. R. Krathwohl. Taxonomy of educational objectives: The classification of\\neducational goals. Book 1, Cognitive domain . longman, 1956.\\n[4]X. Du, J. Shao, and C. Cardie. Learning to ask: Neural question generation for reading\\ncomprehension. arXiv preprint arXiv:1705.00106 , 2017.\\n[5]M. Heilman and N. A. Smith. Good question! statistical ranking for question generation. In\\nHuman language technologies: The 2010 annual conference of the North American Chapter of\\nthe Association for Computational Linguistics , pages 609–617, 2010.\\n[6]E. S. Koç and T. Öntas. A comparative analysis of the 4th and 5th grade social studies curriculum\\naccording to revised bloom taxonomy. Cypriot Journal of Educational Sciences , 15(3):540–553,\\n2020.\\n[7]D. R. Krathwohl. A revision of bloom’s taxonomy: An overview. Theory into practice ,\\n41(4):212–218, 2002.\\n[8]G. Kurdi, J. Leo, B. Parsia, U. Sattler, and S. Al-Emari. A systematic review of automatic\\nquestion generation for educational purposes. International Journal of Artificial Intelligence in\\nEducation , 30:121–204, 2020.\\n[9]T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein,\\nI. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering\\nresearch. Transactions of the Association for Computational Linguistics , 7:453–466, 2019.\\n[10] J. Leinonen, P. Denny, S. MacNeil, S. Sarsa, S. Bernstein, J. Kim, A. Tran, and A. Hellas.\\nComparing code explanations created by students and large language models. arXiv preprint\\narXiv:2304.03938 , 2023.\\n[11] X. Li, P. Yu, C. Zhou, T. Schick, L. Zettlemoyer, O. Levy, J. Weston, and M. Lewis. Self-\\nalignment with instruction backtranslation. arXiv preprint arXiv:2308.06259 , 2023.\\n[12] X. Li, T. Zhang, Y . Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto.\\nAlpacaeval: An automatic evaluator of instruction-following models, 2023.\\n[13] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict:\\nA systematic survey of prompting methods in natural language processing. ACM Computing\\nSurveys , 55(9):1–35, 2023.\\n[14] Y . Liu, D. Iter, Y . Xu, S. Wang, R. Xu, and C. Zhu. G-eval: Nlg evaluation using gpt-4 with\\nbetter human alignment. arXiv preprint arXiv:2303.16634 , 2023.\\n[15] T. Muhayimana, L. Kwizera, and M. R. Nyirahabimana.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res.metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ykjHZpoiKfa",
        "outputId": "dcebb98e-b6d1-422d-c088-d7652648647e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'210a3503-748e-4399-850a-a4e87707f064': {'page_label': '1',\n",
              "  'file_name': '9_paper.pdf',\n",
              "  'file_path': '/content/data/9_paper.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 176888,\n",
              "  'creation_date': '2024-08-19',\n",
              "  'last_modified_date': '2024-08-19'},\n",
              " 'e66e6149-b9f0-4978-b319-0cd78f4ca507': {'page_label': '7',\n",
              "  'file_name': '9_paper.pdf',\n",
              "  'file_path': '/content/data/9_paper.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 176888,\n",
              "  'creation_date': '2024-08-19',\n",
              "  'last_modified_date': '2024-08-19'}}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res.source_nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAix9rGsis6_",
        "outputId": "1b1c51d7-3428-4bbc-8727-4b58cb0c7960"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[NodeWithScore(node=TextNode(id_='210a3503-748e-4399-850a-a4e87707f064', embedding=None, metadata={'page_label': '1', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='feb58b53-c358-4835-ba9e-adb4f2acb02b', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, hash='51ff496876eb576301f2ab9e5fc99dd9948b9cf64833754783a1193ea0e87ffc')}, text='Angel: A New Generation Tool for Learning Material\\nbased Questions and Answers\\nAriel Blobstein∗\\nBen-Gurion University\\narielblo@post.bgu.ac.ilDaniel Izmaylov∗\\nBen-Gurion University\\nizmaylov@post.bgu.ac.il\\nTal Yifal\\nData Innovation Lab\\ntal@datainnovationlab.aiMichal Levy\\nTel Aviv University\\nmichalyot@gmail.comAvi Segal\\nBen-Gurion University\\navise@post.bgu.ac.il\\nAbstract\\nCreating high quality questions and answers for educational purposes continues to\\nbe a challenge for educators and publishers. Past attempts to address this through\\nautomatic generation have shown limited abilities to generate questions targeting\\nhigh cognitive levels, control question complexity and difficulty, or create adequate\\nquestion-answer pairs. We take first steps toward addressing these limitations\\nby introducing a new approach, named Angel, informed by recent developments\\nin Large Language Models and Generative AI. Relying on advanced prompting\\ntechniques, automatic curation, and the incorporation of educational theory into\\nprompts, Angel focuses on generating question answer pairs of varied difficulty\\nwhile targeting higher cognitive levels. Questions and answers are automatically\\ngenerated based on a textbook extract, with Bloom Taxonomy serving as a guide\\nto the creation of questions addressing a diverse set of learning objectives. Our\\nexperiments compare Angel to several baselines and demonstrate the potential of\\ninformed generative models to create high-quality question answer pairs that cover\\na diverse range of cognitive skills.\\n1 Introduction\\nGenerating high-quality questions and answers is of paramount importance in the educational sector.\\nExam-style questions serve as an essential educational tool for assessment as well as a catalyst\\nfor student learning. They provide opportunities for students to practice retrieving information,\\nfocus attention on key learning materials, reinforce learning through repetition of core concepts, and\\nmotivate engagement in learning activities [ 21]. However, manually creating these questions is a\\ncomplex task, requiring significant resources, training, and experience. Automatic questions and\\nanswer generation techniques have emerged as a potential solution to these challenges, offering the\\npossibility of constructing high-quality questions efficiently and cost-effectively. However, until\\nrecently, these techniques have primarily employed a template-based methods. Past reviews highlight\\nthe limitations of these approaches, particularly their inability to generate questions that target high\\ncognitive levels, control question complexity and difficulty, or provide constructive feedback to\\nthe learner [ 26]. Furthermore, these approaches have been constrained by their tendency to rely\\non the existence of a pre-formulated answer for the generated question [ 4,5]. Recent advances in\\nGenerative AI, known for their exceptional text generation capabilities, offer still untapped potential\\nfor automatic question and answer generation [2].\\n∗Equal contribution: These authors contributed equally to this work.\\nNeurIPS’23 Workshop on Generative AI for Education (GAIED).', mimetype='text/plain', start_char_idx=0, end_char_idx=3123, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.605921071296362),\n",
              " NodeWithScore(node=TextNode(id_='e66e6149-b9f0-4978-b319-0cd78f4ca507', embedding=None, metadata={'page_label': '7', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4ed650e8-4ef8-4bfa-ba7b-109fc0a26f79', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '7', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, hash='7556e655998f2d7ea451fe13a3106a4f881bb56d82221891fd4c768dbb7a8420'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='642ba759-edbf-4a52-9c6f-8735b01e412d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5e7f8dc0cffc08f509353ab9c15e6096e604e94080c8b44db9e7ce25e6c73eb2')}, text='when augmented with domain expertise. Future work should focus on increasing sample size to check\\nfor statistical significance, providing better human curated examples to the generative process to\\ndemonstrate higher cognitively complex questions, and using multiple LLMs for LLM-based scoring\\nto alleviate for possible bias introduced by a single model.\\nReferences\\n[1]C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins. Synthetic qa corpora generation with\\nroundtrip consistency. arXiv preprint arXiv:1906.05416 , 2019.\\n[2]D. Baidoo-Anu and L. O. Ansah. Education in the era of generative artificial intelligence (ai):\\nUnderstanding the potential benefits of chatgpt in promoting teaching and learning. Journal of\\nAI, 7(1):52–62, 2023.\\n[3]B. S. Bloom and D. R. Krathwohl. Taxonomy of educational objectives: The classification of\\neducational goals. Book 1, Cognitive domain . longman, 1956.\\n[4]X. Du, J. Shao, and C. Cardie. Learning to ask: Neural question generation for reading\\ncomprehension. arXiv preprint arXiv:1705.00106 , 2017.\\n[5]M. Heilman and N. A. Smith. Good question! statistical ranking for question generation. In\\nHuman language technologies: The 2010 annual conference of the North American Chapter of\\nthe Association for Computational Linguistics , pages 609–617, 2010.\\n[6]E. S. Koç and T. Öntas. A comparative analysis of the 4th and 5th grade social studies curriculum\\naccording to revised bloom taxonomy. Cypriot Journal of Educational Sciences , 15(3):540–553,\\n2020.\\n[7]D. R. Krathwohl. A revision of bloom’s taxonomy: An overview. Theory into practice ,\\n41(4):212–218, 2002.\\n[8]G. Kurdi, J. Leo, B. Parsia, U. Sattler, and S. Al-Emari. A systematic review of automatic\\nquestion generation for educational purposes. International Journal of Artificial Intelligence in\\nEducation , 30:121–204, 2020.\\n[9]T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein,\\nI. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering\\nresearch. Transactions of the Association for Computational Linguistics , 7:453–466, 2019.\\n[10] J. Leinonen, P. Denny, S. MacNeil, S. Sarsa, S. Bernstein, J. Kim, A. Tran, and A. Hellas.\\nComparing code explanations created by students and large language models. arXiv preprint\\narXiv:2304.03938 , 2023.\\n[11] X. Li, P. Yu, C. Zhou, T. Schick, L. Zettlemoyer, O. Levy, J. Weston, and M. Lewis. Self-\\nalignment with instruction backtranslation. arXiv preprint arXiv:2308.06259 , 2023.\\n[12] X. Li, T. Zhang, Y . Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto.\\nAlpacaeval: An automatic evaluator of instruction-following models, 2023.\\n[13] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict:\\nA systematic survey of prompting methods in natural language processing. ACM Computing\\nSurveys , 55(9):1–35, 2023.\\n[14] Y . Liu, D. Iter, Y . Xu, S. Wang, R. Xu, and C. Zhu. G-eval: Nlg evaluation using gpt-4 with\\nbetter human alignment. arXiv preprint arXiv:2303.16634 , 2023.\\n[15] T. Muhayimana, L. Kwizera, and M. R. Nyirahabimana.', mimetype='text/plain', start_char_idx=0, end_char_idx=3104, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5818067322602245)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pmqh5Vbg_g32"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrILqZ3-EMdX",
        "outputId": "b4618582-c82b-4228-9a21-339e23ddaee5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_eJQZbrEuuH",
        "outputId": "de563134-09ac-4aa5-c6a9-18ce4ebebed3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "# Load evaluation metrics using the evaluate library\n",
        "rouge = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "75cf1e982b044c09b84462a75b1309a1",
            "469745936b2846a998ffcda8015b7391",
            "3108e88b6be841988cd3df0629789c79",
            "463d441c1b6a4f16816fa81440b18593",
            "4023ee8e554d4b4d9e73e2c0eb63b97c",
            "712d72ab6eed41389b2d9111aeeed42c",
            "ea45a082adec4483b900adc0c6b6dc2b",
            "66014ce9a8b04772b1f8761d2f93492e",
            "dc2bd4bfd93e46528b6eec7a472856f4",
            "d5b29cf484674d06ac51ad019bbcb166",
            "e4efaa120c1a45b39f47a3de592726d9"
          ]
        },
        "id": "xBJOOGwMEMpv",
        "outputId": "096cbee4-0e68-4760-eebf-629845944ad1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75cf1e982b044c09b84462a75b1309a1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = query_engine.query(\"How does the Angel model utilize Bloom's Taxonomy to improve the generation of educational questions and answers?\")"
      ],
      "metadata": {
        "id": "uQDkKeM-FOdy"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ7ahnRkGiy5",
        "outputId": "1597c4c5-42ff-466f-bffb-a3181bfb7204"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(response=\"The Angel model utilizes Bloom's Taxonomy to improve the generation of educational questions and answers by incorporating the taxonomy's learning objectives into the prompts given to the generative LLM model. The model is trained to generate questions that are more likely to elicit higher-level thinking skills, such as applying, analyzing, evaluating, and creating, rather than simply recalling or understanding information. By doing so, the Angel model can generate questions that are more engaging and challenging for students, and that better align with the educational goals of the textbook. Additionally, the model's ability to explain why a particular question is aligned with a specific Bloom's Taxonomy learning objective can help educators understand how the questions are designed to promote deeper learning and understanding.\", source_nodes=[NodeWithScore(node=TextNode(id_='1b94ffe9-4238-4511-b093-e2a00f9b6fc2', embedding=None, metadata={'page_label': '6', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ccca7d9c-1764-46a1-bbc5-5257e11da75d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, hash='2ec3d165440f73a39f8f066674beeab8aea279c21b5893466cb239d6fe036d63')}, text='Figure 1: Bloom Taxonomy Scores\\nbeing blind to the condition). As seen in the figure, the evaluator judged all generated questions\\nfrom all conditions as pedagogically sound, which correlates with the LLM-based scoring. This\\nis by itself a strong finding for automated approaches to question generation. Additionally, the\\nevaluator scores indicated that all generative based approaches outperformed T5 in answer rating and\\nin Bloom Taxonomy level scoring, with AngelRS and Angel getting the highest scores for Bloom\\nTaxonomy level scoring. We note that even for Angel, the human evaluator judged most questions\\nas Remembering questions. Additional work and human scoring is needed to further analyze this\\noutcome.\\nFigure 2: Human Evaluator. Q&A range is [0,1]; Bloom taxonomy range is [1,5].\\n6 Conclusion\\nIn this study we investigate the practical problem of creating questions and answers for educational\\npurposes given an educational textbook. We propose Angel, an informed prompting and curation\\nbased approach which builds on educational theory and question samples. Angel uses a generative\\nLLM model for creating and curating high quality question answer pairs steering the creation process\\ntowards higher Bloom Taxonomy complexity levels. Our evaluations show that Angel outperformed\\nother non-curated and non generative approaches on most of the inspected measures, and point to the\\npotential of such generative endeavours in creating high quality questions and answers, especially\\n6', mimetype='text/plain', start_char_idx=0, end_char_idx=1490, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7112674815106059), NodeWithScore(node=TextNode(id_='91b18d62-2a3a-40b6-ad54-78cc6e526b02', embedding=None, metadata={'page_label': '9', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0b7e9e80-297c-4210-8595-7c4352554b00', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, hash='2a79e07fae1f5711478b6f2c3b46666771a016e00cff92f5d7a9f4afee2a5e24')}, text='A Generation Prompt\\nA.1 Simple Generation Prompt\\nYou are a middle school science teacher. You are given a paragraph from your textbook and\\nyou need to write final exam questions and answers for your students. You are supposed to\\nwrite questions of 3 types: easy, medium, and hard.\\nThe Paragraph: {{Paragraph}}\\nFigure 3: Simple Generation Prompt\\nA.2 ANGEL Generation Prompt\\nYou are a middle school science teacher. You are given a paragraph from your textbook and\\nyou need to write final exam questions and answers for your students. You are supposed to\\nwrite questions of 3 types: Easy, medium and hard.\\nAdditionally, your generated questions should be based on the Bloom Taxonomy. Specifically,\\nyou should create more questions with higher Bloom Taxonomy learning objectives (applying,\\nanalyzing, evaluating, creating) as opposed to questions with lower learning objectives\\n(reading, understanding).\\nFor each generated question, you should specify its main Bloom Taxonomy learning objective\\nand explain why you think this is the question’s main Bloom Taxonomy objective.\\nAssume that the following are the Bloom Taxonomy learning objectives:\\nRemembering: This level involves recalling facts, details, or information. It is the most basic\\nlevel of cognitive skill.\\nUnderstanding: At this level, learners comprehend the meaning of information and can\\nexplain it in their own words. They demonstrate comprehension of concepts and principles.\\nApplying: This level requires the application of knowledge and concepts to solve problems\\nor perform tasks. It involves using information in new and different situations.\\nAnalyzing: Analyzing involves breaking down information into its constituent parts and\\nidentifying patterns, relationships, or connections among them.\\nEvaluating: At this level, learners make judgments about the value or quality of ideas,\\nsolutions, or arguments. They can critique, assess, and defend their positions.\\nCreating: This is the highest level of cognitive skill, where learners can generate new ideas,\\nconcepts, or products. They can synthesize information from different sources to create\\nsomething new.\\nYou can see under \"Examples:\" below a few examples of past questions created for this\\nparagraph. Please provide each answer with a step-by-step thinking for why the specific\\nanswer is the right answer.\\nExamples: {Examples}\\nThe Paragraph: {Paragraph}\\nFigure 4: Angel Generation Prompt\\n9', mimetype='text/plain', start_char_idx=0, end_char_idx=2412, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6615953256655843)], metadata={'1b94ffe9-4238-4511-b093-e2a00f9b6fc2': {'page_label': '6', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}, '91b18d62-2a3a-40b6-ad54-78cc6e526b02': {'page_label': '9', 'file_name': '9_paper.pdf', 'file_path': '/content/data/9_paper.pdf', 'file_type': 'application/pdf', 'file_size': 176888, 'creation_date': '2024-08-19', 'last_modified_date': '2024-08-19'}})"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res.metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zlq_qUQA3_9",
        "outputId": "585e0490-71e6-4c00-d672-fb4b5bbf7e4c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1b94ffe9-4238-4511-b093-e2a00f9b6fc2': {'page_label': '6',\n",
              "  'file_name': '9_paper.pdf',\n",
              "  'file_path': '/content/data/9_paper.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 176888,\n",
              "  'creation_date': '2024-08-19',\n",
              "  'last_modified_date': '2024-08-19'},\n",
              " '91b18d62-2a3a-40b6-ad54-78cc6e526b02': {'page_label': '9',\n",
              "  'file_name': '9_paper.pdf',\n",
              "  'file_path': '/content/data/9_paper.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 176888,\n",
              "  'creation_date': '2024-08-19',\n",
              "  'last_modified_date': '2024-08-19'}}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Test_case_1 = res.metadata"
      ],
      "metadata": {
        "id": "8l_KV3ORA98A"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Test_case_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GWuoJgnBG-J",
        "outputId": "6baa0182-db55-4c6d-c327-522564143dd2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1b94ffe9-4238-4511-b093-e2a00f9b6fc2': {'page_label': '6',\n",
              "  'file_name': '9_paper.pdf',\n",
              "  'file_path': '/content/data/9_paper.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 176888,\n",
              "  'creation_date': '2024-08-19',\n",
              "  'last_modified_date': '2024-08-19'},\n",
              " '91b18d62-2a3a-40b6-ad54-78cc6e526b02': {'page_label': '9',\n",
              "  'file_name': '9_paper.pdf',\n",
              "  'file_path': '/content/data/9_paper.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 176888,\n",
              "  'creation_date': '2024-08-19',\n",
              "  'last_modified_date': '2024-08-19'}}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_response = res.response"
      ],
      "metadata": {
        "id": "J9A2eW4LFjXS"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "ISBTeUPwGOhI",
        "outputId": "d225b2c3-2491-402a-9297-f4a3084e130d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Angel model utilizes Bloom's Taxonomy to improve the generation of educational questions and answers by incorporating the taxonomy's learning objectives into the prompts given to the generative LLM model. The model is trained to generate questions that are more likely to elicit higher-level thinking skills, such as applying, analyzing, evaluating, and creating, rather than simply recalling or understanding information. By doing so, the Angel model can generate questions that are more engaging and challenging for students, and that better align with the educational goals of the textbook. Additionally, the model's ability to explain why a particular question is aligned with a specific Bloom's Taxonomy learning objective can help educators understand how the questions are designed to promote deeper learning and understanding.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input=\"How does the Angel model utilize Bloom's Taxonomy to improve the generation of educational questions and answers?\"\n",
        "actual_output= generated_response\n",
        "expected_output=\"The Angel model utilizes Bloom's Taxonomy to enhance the generation of educational questions and answers by structuring the creation process around different levels of cognitive complexity. Specifically, the model is instructed to generate questions that align with various levels of Bloom's Taxonomy, from basic recall of facts (Remembering) to higher-order skills such as analyzing, evaluating, and creating. This is achieved through a specialized prompt that guides the model to focus on producing questions that address these cognitive levels, particularly aiming for the higher levels of Bloom's Taxonomy. By incorporating this taxonomy, the Angel model is able to create a diverse set of questions that not only test basic knowledge but also encourage deeper cognitive engagement from learners, making the generated content more educationally valuable​\",\n"
      ],
      "metadata": {
        "id": "fR8U2fleFA0M"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Calculate and print ROUGE score\n",
        "rouge_score = rouge.compute(predictions=[generated_response], references=expected_output)\n",
        "print(f\"ROUGE Score: {rouge_score}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZgPrUK_EMub",
        "outputId": "75a7cebb-beba-4ea4-f233-3e4e47cc60ac"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Score: {'rouge1': 0.4862745098039216, 'rouge2': 0.25296442687747034, 'rougeL': 0.36078431372549025, 'rougeLsum': 0.36078431372549025}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_case_2 = query_engine.query(\"What are the key benefits of using large language models (LLMs) for classifying student help requests in programming courses?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoZBEmp49ESD",
        "outputId": "9e55c6e1-b94f-4ab7-dde8-b8510ec7767e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_response = test_case_2.response"
      ],
      "metadata": {
        "id": "SoZPpDa-9oeB"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_case_2.metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDVGj4SuAwY-",
        "outputId": "c951d03e-9b78-4525-bacb-74066f08effd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'08f8e1db-0c80-42b6-84b5-8aed80bf8c3b': {'page_label': '1',\n",
              "  'file_name': '15_paper.pdf',\n",
              "  'file_path': '/content/data/15_paper.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 412356,\n",
              "  'creation_date': '2024-08-19',\n",
              "  'last_modified_date': '2024-08-19'},\n",
              " 'fa090e16-64df-47d6-aabb-134c9490ff71': {'page_label': '6',\n",
              "  'file_name': '15_paper.pdf',\n",
              "  'file_path': '/content/data/15_paper.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 412356,\n",
              "  'creation_date': '2024-08-19',\n",
              "  'last_modified_date': '2024-08-19'}}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "DLS0uzVd-cTG",
        "outputId": "ff3981c8-950b-4371-fb98-6006ce0661cd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The key benefits of using large language models (LLMs) for classifying student help requests in programming courses are:\\n\\n1. Accessibility: LLMs are hosted and available via APIs, requiring little to no local infrastructure and relatively little technical expertise.\\n2. Cost-effectiveness: LLMs are more accessible than using other ML-powered methods, with a low barrier to experimenting with many different labeling schemes.\\n3. Ease of integration: LLMs can be easily integrated with existing systems, allowing tailoring a system to specific educational contexts as well as rapid iterative improvement of existing schemes.\\n4. Improved accuracy: Fine-tuning a GPT-3.5 model on a small amount of labeled data greatly improved its performance, reaching human-level accuracy.\\n5. Personalized and scalable assistance: Automated systems that accurately classify student queries can provide tailored and effective responses to students and deliver insights to educators about how students are interacting with such tools.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input=\"What are the key benefits of using large language models (LLMs) for classifying student help requests in programming courses?\"\n",
        "actual_output= generated_response\n",
        "expected_output='''The key benefits of using large language models (LLMs) like GPT-3.5 and GPT-4 for classifying student help requests in programming courses are as follows:\n",
        "\n",
        "Scalability and Efficiency: LLMs can process and classify large volumes of student queries quickly and with minimal human intervention, making them highly scalable solutions for courses with large enrollments. This efficiency is particularly valuable in introductory programming courses where the student-to-instructor ratio is high.\n",
        "\n",
        "Accuracy and Consistency: Fine-tuning LLMs on a specific dataset can improve their performance to a level comparable with human raters, ensuring accurate and consistent classification of student queries. This allows for more reliable responses to student help requests, enhancing the overall learning experience.\n",
        "\n",
        "Cost-Effectiveness: Using LLMs for classification is cost-effective compared to traditional machine learning methods that require extensive labeled datasets and computational resources. LLMs can achieve high accuracy even with limited labeled data, reducing the cost associated with data labeling and model training.\n",
        "\n",
        "Flexibility and Adaptability: LLMs are highly flexible and can be easily adapted to different educational contexts or modified labeling schemes. This adaptability makes them suitable for a wide range of educational applications, from programming courses to other disciplines.\n",
        "\n",
        "Real-Time Insights for Educators: By automatically classifying student queries, LLMs can provide educators with real-time aggregated information about student questions and help-seeking behaviors. This data can be used to identify trends, such as a shift in the types of queries being asked, and to intervene when necessary to support student learning more effectively​'''"
      ],
      "metadata": {
        "id": "DAemHiBf9og3"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print ROUGE score\n",
        "rouge_score = rouge.compute(predictions=[generated_response], references=[expected_output])\n",
        "print(f\"ROUGE Score: {rouge_score}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdP2A4jP9olK",
        "outputId": "e40580dc-d004-4ce2-ae19-a2b0837effd7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Score: {'rouge1': 0.43672456575682383, 'rouge2': 0.1596009975062344, 'rougeL': 0.23325062034739452, 'rougeLsum': 0.37717121588089336}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del input"
      ],
      "metadata": {
        "id": "RTDn8sil_Usz"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input =\"\"\n",
        "user_input = input(\"Enter your question or exit: \")\n",
        "while user_input != \"exit\":\n",
        "  response = query_engine.query(user_input)\n",
        "  print(response)\n",
        "  print(\"\\n ############## \\n\")\n",
        "  print(response.metadata)\n",
        "  print(\"\\n ############## \\n\")\n",
        "  user_input = input(\"Enter your question or exit: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08IZBmo4retP",
        "outputId": "0963e66e-0240-4c41-fd0e-a320e8f51079"
      },
      "execution_count": 62,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question or exit: exit\n"
          ]
        }
      ]
    }
  ]
}